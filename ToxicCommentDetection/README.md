# Sistema de DetecciÃ³n de Comentarios TÃ³xicos con LLMs

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.2%2B-orange)
![Transformers](https://img.shields.io/badge/Transformers-4.30%2B-yellowgreen)

Este sistema clasifica comentarios como tÃ³xicos o no tÃ³xicos utilizando **embeddings generados por un modelo LLM (DistilRoBERTa)**, seguido de un clasificador tradicional (`RandomForestClassifier`).  
EstÃ¡ basado en la competencia de Kaggle:  
ðŸ”— [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification)

---

## Estructura del Proyecto

```text
ToxicCommentDetection/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Datos originales (deben colocarse manualmente)
â”‚   â””â”€â”€ processed/          # Datos procesados automÃ¡ticamente
â”œâ”€â”€ models/                 # Modelos entrenados y predicciones
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ modules/            # MÃ³dulos personalizados (TextCleaner, ContextAnalyzer, etc.)
â”‚   â”œâ”€â”€ config.py           # ConfiguraciÃ³n general del sistema
â”‚   â”œâ”€â”€ dependency_checker.py
â”‚   â”œâ”€â”€ evaluation.py       # MÃ©tricas y anÃ¡lisis
â”‚   â”œâ”€â”€ main.py             # Punto de entrada principal
â”‚   â”œâ”€â”€ preprocessing.py    # Preprocesamiento y balanceo
â”‚   â””â”€â”€ test_cases.py       # Casos de prueba crÃ­ticos
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup_resources.py      # Script para descarga inicial (si aplica)
â””â”€â”€ README.md               # Este archivo
```

---

## Requisitos Previos

- Python **3.10** o superior
- Instalar las dependencias necesarias ejecutando:

```bash
pip install -r requirements.txt
```

- Descargar los archivos de datos desde la competencia en Kaggle y colocarlos en la carpeta `data/raw/`:

```text
data/raw/
â”œâ”€â”€ train.csv
â”œâ”€â”€ test.csv
â”œâ”€â”€ identity_individual_annotations.csv
â”œâ”€â”€ sample_submission.csv
```

> AsegÃºrate de que los nombres de los archivos sean exactamente los mencionados arriba.

---

## ðŸš€ Instrucciones de EjecuciÃ³n

1. **Clonar el repositorio:**

```bash
git clone https://github.com/tuusuario/ToxicCommentDetection.git
cd ToxicCommentDetection
```

2. **Instalar dependencias:**

```bash
pip install -r requirements.txt
```

3. **Colocar los archivos de datos** en `data/raw/` (ver secciÃ³n anterior).

4. **Ejecutar el sistema:**

```bash
python src/main.py
```

---

## ðŸ“¦ Resultados Generados

- Modelo entrenado con embeddings LLM:  
  `models/llm_toxicity_model.joblib`

- Predicciones para envÃ­o en Kaggle:  
  `models/kaggle_submission.csv`

- PrecisiÃ³n crÃ­tica esperada:  
  > Mayor al **90%** en casos ambiguos (irÃ³nicos, sarcÃ¡sticos, o con identidades sensibles)

---

## ðŸ” Flujo de Trabajo del Sistema

```mermaid
graph LR
    A[Comentario crudo] --> B[TextCleaner]
    B --> C[Embeddings: DistilRoBERTa]
    C --> D[Clasificador: RandomForest]
    D --> E[PredicciÃ³n de toxicidad]
```

---

## ðŸ§  Detalles TÃ©cnicos

- **Embeddings**: `distilroberta-base` usando la librerÃ­a `sentence-transformers`
- **Clasificador**: `RandomForestClassifier` con balanceo de clases
- **Procesamiento optimizado**: para CPU (soporta chunks grandes)
- **Robustez**: el sistema detecta sarcasmo, negaciones, e identidades sensibles sin reglas manuales

---

## ðŸ‘¥ Autores

- Andrey Camilo Gonzalez Caceres  
- Hugo Mojica Angarita  
- Laura Paez Cifuentes  
---

Universidad Distrital Francisco JosÃ© de Caldas  
Systems Analysis & Design - 2025

## Enlace a la competencia

[Jigsaw Unintended Bias in Toxicity Classification â€“ Kaggle](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification)



Universidad Distrital Francisco JosÃ© de Caldas  
Systems Analysis & Design - 2025
