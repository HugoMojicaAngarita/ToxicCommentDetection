# Toxic Comment Detection System with LLMs

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.2%2B-orange)
![Transformers](https://img.shields.io/badge/Transformers-4.30%2B-yellowgreen)

This system classifies comments as toxic or non-toxic using **embeddings generated by an LLM model (DistilRoBERTa)**, followed by a traditional classifier (`RandomForestClassifier`).  
It is based on the Kaggle competition:  
ðŸ”— [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification)

---

## Project Structure

```text
ToxicCommentDetection/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Original data (must be placed manually)
â”‚   â””â”€â”€ processed/          # Automatically generated processed data
â”œâ”€â”€ models/                 # Trained models and predictions
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ modules/            # Custom modules (TextCleaner, ContextAnalyzer, etc.)
â”‚   â”œâ”€â”€ config.py           # Global configuration
â”‚   â”œâ”€â”€ dependency_checker.py
â”‚   â”œâ”€â”€ evaluation.py       # Metrics and analysis
â”‚   â”œâ”€â”€ main.py             # Main entry point
â”‚   â”œâ”€â”€ preprocessing.py    # Preprocessing and balancing
â”‚   â””â”€â”€ test_cases.py       # Critical test cases
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup_resources.py      # Optional setup script
â””â”€â”€ README.md               # This file

```

---

## Prerequisites

- Python **3.10** or higher
- Install the required dependencies:

```bash
pip install -r requirements.txt
```

- Download the competition files from Kaggle and place them in the `data/raw/` folder:

```text
data/raw/
â”œâ”€â”€ train.csv
â”œâ”€â”€ test.csv
â”œâ”€â”€ identity_individual_annotations.csv
â”œâ”€â”€ sample_submission.csv
```

> Ensure the filenames match exactly as shown.

---

## Execution Instructions

1. **Clone the repository:**

```bash
git clone https://github.com/yourusername/ToxicCommentDetection.git
cd ToxicCommentDetection
```

2. **Install dependencies:**

```bash
pip install -r requirements.txt
```

3. **Place the dataset files in** `data/raw/` as described above.

4. **Run the system:**

```bash
python src/main.py
```

---

## Generated Outputs

- Trained LLM-based model:  
  `models/llm_toxicity_model.joblib`

- Predictions for Kaggle submission:  
  `models/kaggle_submission.csv`

- Expected critical-case accuracy:
  > Above **90%** in ambiguous cases (sarcasm, irony, identity-based content)

---

## System Workflow

```mermaid
graph LR
    A[Raw comment] --> B[TextCleaner]
    B --> C[Embeddings: DistilRoBERTa]
    C --> D[Classifier: RandomForest]
    D --> E[Toxicity prediction]

```

---

## Technical Details

- **Embeddings**: `distilroberta-base` via the `sentence-transformers` library
- **Classifier**: `RandomForestClassifier` with class balancing
- **Optimized processing**: CPU-friendly, supports large datasets
- **Robustness**: Capable of detecting sarcasm, negations, and identity references without manual rules

---

## Authors

- Andrey Camilo Gonzalez Caceres  
- Hugo Mojica Angarita  
- Laura Paez Cifuentes  
---


## Link

[Jigsaw Unintended Bias in Toxicity Classification â€“ Kaggle](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification)
https://www.kaggle.com/code/hugomojicaangarita/datatoxicity

Universidad Distrital Francisco JosÃ© de Caldas  
Systems Analysis & Design - 2025
